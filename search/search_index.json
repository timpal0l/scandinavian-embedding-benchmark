{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scandinavian Embedding Benchmark","text":"<p>This is the documentation for the Scandinavian Embedding Benchmark. This benchmark is intended to evaluate the sentence/document embeddings of large language models.</p> <p>Intended uses for this benchmark:</p> <ul> <li>Evaluating document embeddings of Scandinavian language models</li> <li>Evaluating document embeddings for multilingual models on Scandinavian languages</li> <li>Allow ranking of competing Scandinavian and multilingual models using no more compute than what a consumer laptop can provide </li> </ul> AllDanishNorwegianSwedish <p> </p> <p></p> <p></p> <p></p>"},{"location":"#comparison-to-other-benchmarks","title":"Comparison to other benchmarks","text":"<p>If you use this benchmark for a relative ranking of language models were you plan to fine-tune the models I would recommend looking at ScandEval, which as benchmarks the model using a cross-validated fine-tuning. It also includes structured prediction tasks such as named entity recognition. Many of the tasks in this embedding benchmark are also included in ScandEval and an attempt have been made to use the same versions of the tasks. There is a few tasks (ScandiQA) which are included in ScandEval, but not in this benchmark as they are human translation of an English dataset.</p> <p>The tasks within this benchmark are also included in the MTEB leaderboard, though the aggregation methods very slightly. MTEB is primarily an English embedding benchmark, with a few multilingual tasks and additional languages. As a part of this project, the tasks were also added to the MTEB leaderboard.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#general","title":"General","text":"<p>General function for dealing with tasks and models implemented in SEB.</p>"},{"location":"api/#seb.get_task","title":"<code>seb.get_task(name)</code>","text":"<p>Fetches a task by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the task.</p> required <p>Returns:</p> Type Description <code>Task</code> <p>A task.</p> Source code in <code>src/seb/registries.py</code> <pre><code>def get_task(name: str) -&gt; Task:\n\"\"\"\n    Fetches a task by name.\n\n    Args:\n        name: The name of the task.\n\n    Returns:\n        A task.\n    \"\"\"\n    return tasks.get(name)()\n</code></pre>"},{"location":"api/#seb.get_all_tasks","title":"<code>seb.get_all_tasks()</code>","text":"<p>Returns all tasks implemented in SEB.</p> <p>Returns:</p> Type Description <code>list[Task]</code> <p>A list of all tasks in SEB.</p> Source code in <code>src/seb/registries.py</code> <pre><code>def get_all_tasks() -&gt; list[Task]:\n\"\"\"\n    Returns all tasks implemented in SEB.\n\n    Returns:\n        A list of all tasks in SEB.\n    \"\"\"\n    return [get_task(task_name) for task_name in tasks.get_all()]\n</code></pre>"},{"location":"api/#seb.get_model","title":"<code>seb.get_model(name)</code>","text":"<p>Fetches a model by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the model.</p> required <p>Returns:</p> Type Description <code>SebModel</code> <p>A model including metadata.</p> Source code in <code>src/seb/registries.py</code> <pre><code>def get_model(name: str) -&gt; SebModel:\n\"\"\"\n    Fetches a model by name.\n\n    Args:\n        name: The name of the model.\n\n    Returns:\n        A model including metadata.\n    \"\"\"\n    return models.get(name)()\n</code></pre>"},{"location":"api/#seb.get_all_models","title":"<code>seb.get_all_models()</code>","text":"<p>Get all the models implemented in SEB.</p> <p>Returns:</p> Type Description <code>list[SebModel]</code> <p>A list of all models in SEB.</p> Source code in <code>src/seb/registries.py</code> <pre><code>def get_all_models() -&gt; list[SebModel]:\n\"\"\"\n    Get all the models implemented in SEB.\n\n    Returns:\n        A list of all models in SEB.\n    \"\"\"\n    return [get_model(model_name) for model_name in models.get_all()]\n</code></pre>"},{"location":"api/#benchmark","title":"Benchmark","text":""},{"location":"api/#seb.Benchmark","title":"<code>seb.Benchmark</code>","text":"<p>Benchmark is the main orchestrator of the SEB benchmark.</p> Source code in <code>src/seb/benchmark.py</code> <pre><code>class Benchmark:\n\"\"\"\n    Benchmark is the main orchestrator of the SEB benchmark.\n    \"\"\"\n\n    def __init__(\n        self,\n        languages: Optional[list[str]] = None,\n        tasks: Optional[list[str]] = None,\n    ) -&gt; None:\n\"\"\"\n        Initialize the benchmark.\n\n        Args:\n            languages: A list of languages to run the benchmark on. If None, all languages are used.\n            tasks: A list of tasks to run the benchmark on. If None, all tasks are used.\n        \"\"\"\n        self.languages = languages\n        self.tasks_names = tasks\n        self.tasks = self.get_tasks()\n\n    def get_tasks(self) -&gt; list[Task]:\n\"\"\"\n        Get the tasks for the benchmark.\n\n        Returns:\n            A list of tasks.\n        \"\"\"\n        tasks = []\n\n        if self.tasks_names is not None:\n            tasks: list[Task] = [get_task(task_name) for task_name in self.tasks_names]\n        else:\n            tasks: list[Task] = get_all_tasks()\n\n        if self.languages is not None:\n            langs = set(self.languages)\n            tasks = [task for task in tasks if set(task.languages) &amp; langs]\n\n        return tasks\n\n    def evaluate_model(\n        self,\n        model: SebModel,\n        use_cache: bool = True,\n        raise_errors: bool = True,\n    ) -&gt; BenchmarkResults:\n\"\"\"\n        Evaluate a model on the benchmark.\n\n        Args:\n            model: The model to evaluate.\n            use_cache: Whether to use the cache.\n            raise_errors: Whether to raise errors.\n\n        Returns:\n            The results of the benchmark.\n        \"\"\"\n        tasks = self.get_tasks()\n        task_results = []\n        pbar = tqdm(tasks, position=1, desc=f\"Running {model.meta.name}\", leave=False)\n        for task in pbar:\n            task_result = run_task(task, model, use_cache, raise_errors)\n            task_results.append(task_result)\n\n        return BenchmarkResults(meta=model.meta, task_results=task_results)\n\n    def evaluate_models(\n        self,\n        models: list[SebModel],\n        use_cache: bool = True,\n        raise_errors: bool = True,\n    ) -&gt; list[BenchmarkResults]:\n\"\"\"\n        Evaluate a list of models on the benchmark.\n\n        Args:\n            models: The models to evaluate.\n            use_cache: Whether to use the cache.\n            raise_errors: Whether to raise errors.\n\n        Returns:\n            The results of the benchmark, once for each model.\n        \"\"\"\n        results = []\n        pbar = tqdm(models, position=0, desc=\"Running Benchmark\", leave=True)\n\n        for model in pbar:\n            results.append(\n                self.evaluate_model(\n                    model,\n                    use_cache=use_cache,\n                    raise_errors=raise_errors,\n                ),\n            )\n        return results\n</code></pre>"},{"location":"api/#seb.Benchmark.__init__","title":"<code>__init__(languages=None, tasks=None)</code>","text":"<p>Initialize the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>languages</code> <code>Optional[list[str]]</code> <p>A list of languages to run the benchmark on. If None, all languages are used.</p> <code>None</code> <code>tasks</code> <code>Optional[list[str]]</code> <p>A list of tasks to run the benchmark on. If None, all tasks are used.</p> <code>None</code> Source code in <code>src/seb/benchmark.py</code> <pre><code>def __init__(\n    self,\n    languages: Optional[list[str]] = None,\n    tasks: Optional[list[str]] = None,\n) -&gt; None:\n\"\"\"\n    Initialize the benchmark.\n\n    Args:\n        languages: A list of languages to run the benchmark on. If None, all languages are used.\n        tasks: A list of tasks to run the benchmark on. If None, all tasks are used.\n    \"\"\"\n    self.languages = languages\n    self.tasks_names = tasks\n    self.tasks = self.get_tasks()\n</code></pre>"},{"location":"api/#seb.Benchmark.evaluate_model","title":"<code>evaluate_model(model, use_cache=True, raise_errors=True)</code>","text":"<p>Evaluate a model on the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SebModel</code> <p>The model to evaluate.</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use the cache.</p> <code>True</code> <code>raise_errors</code> <code>bool</code> <p>Whether to raise errors.</p> <code>True</code> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>The results of the benchmark.</p> Source code in <code>src/seb/benchmark.py</code> <pre><code>def evaluate_model(\n    self,\n    model: SebModel,\n    use_cache: bool = True,\n    raise_errors: bool = True,\n) -&gt; BenchmarkResults:\n\"\"\"\n    Evaluate a model on the benchmark.\n\n    Args:\n        model: The model to evaluate.\n        use_cache: Whether to use the cache.\n        raise_errors: Whether to raise errors.\n\n    Returns:\n        The results of the benchmark.\n    \"\"\"\n    tasks = self.get_tasks()\n    task_results = []\n    pbar = tqdm(tasks, position=1, desc=f\"Running {model.meta.name}\", leave=False)\n    for task in pbar:\n        task_result = run_task(task, model, use_cache, raise_errors)\n        task_results.append(task_result)\n\n    return BenchmarkResults(meta=model.meta, task_results=task_results)\n</code></pre>"},{"location":"api/#seb.Benchmark.evaluate_models","title":"<code>evaluate_models(models, use_cache=True, raise_errors=True)</code>","text":"<p>Evaluate a list of models on the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list[SebModel]</code> <p>The models to evaluate.</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use the cache.</p> <code>True</code> <code>raise_errors</code> <code>bool</code> <p>Whether to raise errors.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[BenchmarkResults]</code> <p>The results of the benchmark, once for each model.</p> Source code in <code>src/seb/benchmark.py</code> <pre><code>def evaluate_models(\n    self,\n    models: list[SebModel],\n    use_cache: bool = True,\n    raise_errors: bool = True,\n) -&gt; list[BenchmarkResults]:\n\"\"\"\n    Evaluate a list of models on the benchmark.\n\n    Args:\n        models: The models to evaluate.\n        use_cache: Whether to use the cache.\n        raise_errors: Whether to raise errors.\n\n    Returns:\n        The results of the benchmark, once for each model.\n    \"\"\"\n    results = []\n    pbar = tqdm(models, position=0, desc=\"Running Benchmark\", leave=True)\n\n    for model in pbar:\n        results.append(\n            self.evaluate_model(\n                model,\n                use_cache=use_cache,\n                raise_errors=raise_errors,\n            ),\n        )\n    return results\n</code></pre>"},{"location":"api/#seb.Benchmark.get_tasks","title":"<code>get_tasks()</code>","text":"<p>Get the tasks for the benchmark.</p> <p>Returns:</p> Type Description <code>list[Task]</code> <p>A list of tasks.</p> Source code in <code>src/seb/benchmark.py</code> <pre><code>def get_tasks(self) -&gt; list[Task]:\n\"\"\"\n    Get the tasks for the benchmark.\n\n    Returns:\n        A list of tasks.\n    \"\"\"\n    tasks = []\n\n    if self.tasks_names is not None:\n        tasks: list[Task] = [get_task(task_name) for task_name in self.tasks_names]\n    else:\n        tasks: list[Task] = get_all_tasks()\n\n    if self.languages is not None:\n        langs = set(self.languages)\n        tasks = [task for task in tasks if set(task.languages) &amp; langs]\n\n    return tasks\n</code></pre>"},{"location":"api/#model-interface","title":"Model Interface","text":""},{"location":"api/#seb.ModelInterface","title":"<code>seb.ModelInterface</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Interface which all models must implement.</p> Source code in <code>src/seb/model_interface.py</code> <pre><code>@runtime_checkable\nclass ModelInterface(Protocol):\n\"\"\"\n    Interface which all models must implement.\n    \"\"\"\n\n    def encode(\n        self,\n        sentences: list[str],\n        batch_size: int = 32,\n        **kwargs: dict,\n    ) -&gt; list[ArrayLike]:\n\"\"\"Returns a list of embeddings for the given sentences.\n        Args:\n            sentences: List of sentences to encode\n            batch_size: Batch size for the encoding\n            kwargs: arguments to pass to the models encode method\n\n        Returns:\n            List of embeddings for the given sentences\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/#seb.ModelInterface.encode","title":"<code>encode(sentences, batch_size=32, **kwargs)</code>","text":"<p>Returns a list of embeddings for the given sentences. Args:     sentences: List of sentences to encode     batch_size: Batch size for the encoding     kwargs: arguments to pass to the models encode method</p> <p>Returns:</p> Type Description <code>list[ArrayLike]</code> <p>List of embeddings for the given sentences</p> Source code in <code>src/seb/model_interface.py</code> <pre><code>def encode(\n    self,\n    sentences: list[str],\n    batch_size: int = 32,\n    **kwargs: dict,\n) -&gt; list[ArrayLike]:\n\"\"\"Returns a list of embeddings for the given sentences.\n    Args:\n        sentences: List of sentences to encode\n        batch_size: Batch size for the encoding\n        kwargs: arguments to pass to the models encode method\n\n    Returns:\n        List of embeddings for the given sentences\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#task-interface","title":"Task Interface","text":""},{"location":"api/#seb.Task","title":"<code>seb.Task</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>A task is a specific evaluation task for a sentence embedding model.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the task.</p> <code>main_score</code> <code>str</code> <p>The main score of the task.</p> <code>description</code> <code>str</code> <p>A description of the task.</p> <code>reference</code> <code>str</code> <p>A reference to the task.</p> <code>version</code> <code>str</code> <p>The version of the task.</p> <code>languages</code> <code>list[str]</code> <p>The languages of the task.</p> Source code in <code>src/seb/tasks_interface.py</code> <pre><code>@runtime_checkable\nclass Task(Protocol):\n\"\"\"\n    A task is a specific evaluation task for a sentence embedding model.\n\n    Attributes:\n        name: The name of the task.\n        main_score: The main score of the task.\n        description: A description of the task.\n        reference: A reference to the task.\n        version: The version of the task.\n        languages: The languages of the task.\n\n    \"\"\"\n\n    name: str\n    main_score: str\n    description: str\n    reference: str\n    version: str\n    languages: list[str]\n\n    def evaluate(self, model: ModelInterface) -&gt; TaskResult:\n\"\"\"\n        Evaluates a Sentence Embedding Model on the task.\n\n        Args:\n            model: A sentence embedding model.\n\n        Returns:\n            A TaskResult object.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/#seb.Task.evaluate","title":"<code>evaluate(model)</code>","text":"<p>Evaluates a Sentence Embedding Model on the task.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelInterface</code> <p>A sentence embedding model.</p> required <p>Returns:</p> Type Description <code>TaskResult</code> <p>A TaskResult object.</p> Source code in <code>src/seb/tasks_interface.py</code> <pre><code>def evaluate(self, model: ModelInterface) -&gt; TaskResult:\n\"\"\"\n    Evaluates a Sentence Embedding Model on the task.\n\n    Args:\n        model: A sentence embedding model.\n\n    Returns:\n        A TaskResult object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#data-classes","title":"Data Classes","text":"<p>SEB uses data classes to store the results of a benchmark. The following classes are available:</p>"},{"location":"api/#seb.BenchmarkResults","title":"<code>seb.BenchmarkResults</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Dataclass for storing benchmark results.</p> <p>Attributes:</p> Name Type Description <code>meta</code> <code>ModelMeta</code> <p>ModelMeta object.</p> <code>task_results</code> <code>list[Union[TaskResult, TaskError]]</code> <p>List of TaskResult objects.</p> Source code in <code>src/seb/result_dataclasses.py</code> <pre><code>class BenchmarkResults(BaseModel):\n\"\"\"\n    Dataclass for storing benchmark results.\n\n    Attributes:\n        meta: ModelMeta object.\n        task_results: List of TaskResult objects.\n    \"\"\"\n\n    meta: ModelMeta\n    task_results: list[Union[TaskResult, TaskError]]\n\n    def __iter__(self) -&gt; Iterator[Union[TaskResult, TaskError]]:\n        return iter(self.task_results)\n\n    def __getitem__(self, index: int) -&gt; Union[TaskResult, TaskError]:\n        return self.task_results[index]\n\n    def __len__(self) -&gt; int:\n        return len(self.task_results)\n</code></pre>"},{"location":"api/#seb.TaskResult","title":"<code>seb.TaskResult</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Dataclass for storing task results.</p> <p>Attributes:</p> Name Type Description <code>task_name</code> <code>str</code> <p>Name of the task.</p> <code>task_description</code> <code>str</code> <p>Description of the task.</p> <code>task_version</code> <code>str</code> <p>Version of the task.</p> <code>time_of_run</code> <code>datetime</code> <p>Time of the run.</p> <code>scores</code> <code>dict[str, dict[str, float]]</code> <p>Dictionary of scores on the form {language: {\"metric\": value}}.</p> <code>main_score</code> <code>str</code> <p>Name of the main score.</p> Source code in <code>src/seb/result_dataclasses.py</code> <pre><code>class TaskResult(BaseModel):\n\"\"\"\n    Dataclass for storing task results.\n\n    Attributes:\n        task_name: Name of the task.\n        task_description: Description of the task.\n        task_version: Version of the task.\n        time_of_run: Time of the run.\n        scores: Dictionary of scores on the form {language: {\"metric\": value}}.\n        main_score: Name of the main score.\n    \"\"\"\n\n    task_name: str\n    task_description: str\n    task_version: str\n    time_of_run: datetime\n    scores: dict[str, dict[str, float]]  # {language: {\"metric\": value}}.\n    main_score: str\n\n    def get_main_score(self, lang: Optional[Iterable[str]] = None) -&gt; float:\n\"\"\"\n        Returns the main score for a given set of languages.\n\n        Args:\n            lang: List of languages to get the main score for.\n\n        Returns:\n            The main score.\n        \"\"\"\n        main_scores = []\n        if lang is None:\n            lang = self.scores.keys()\n\n        for l in lang:\n            main_scores.append(self.scores[l][self.main_score])\n\n        return sum(main_scores) / len(main_scores)\n\n    @property\n    def languages(self) -&gt; list[str]:\n\"\"\"\n        Returns the languages of the task.\n        \"\"\"\n        return list(self.scores.keys())\n\n    @classmethod\n    def from_disk(cls, path: Path) -&gt; \"TaskResult\":  # noqa: ANN102\n\"\"\"\n        Load task results from a path.\n        \"\"\"\n        with path.open() as f:\n            task_results = json.load(f)\n        return cls(**task_results)\n\n    def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n        Write task results to a path.\n        \"\"\"\n        path.parent.mkdir(parents=True, exist_ok=True)\n        json_str: str = self.model_dump_json()\n\n        with path.open(\"w\") as f:\n            f.write(json_str)\n</code></pre>"},{"location":"api/#seb.TaskResult.languages","title":"<code>languages: list[str]</code>  <code>property</code>","text":"<p>Returns the languages of the task.</p>"},{"location":"api/#seb.TaskResult.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Load task results from a path.</p> Source code in <code>src/seb/result_dataclasses.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path) -&gt; \"TaskResult\":  # noqa: ANN102\n\"\"\"\n    Load task results from a path.\n    \"\"\"\n    with path.open() as f:\n        task_results = json.load(f)\n    return cls(**task_results)\n</code></pre>"},{"location":"api/#seb.TaskResult.get_main_score","title":"<code>get_main_score(lang=None)</code>","text":"<p>Returns the main score for a given set of languages.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>Optional[Iterable[str]]</code> <p>List of languages to get the main score for.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The main score.</p> Source code in <code>src/seb/result_dataclasses.py</code> <pre><code>def get_main_score(self, lang: Optional[Iterable[str]] = None) -&gt; float:\n\"\"\"\n    Returns the main score for a given set of languages.\n\n    Args:\n        lang: List of languages to get the main score for.\n\n    Returns:\n        The main score.\n    \"\"\"\n    main_scores = []\n    if lang is None:\n        lang = self.scores.keys()\n\n    for l in lang:\n        main_scores.append(self.scores[l][self.main_score])\n\n    return sum(main_scores) / len(main_scores)\n</code></pre>"},{"location":"api/#seb.TaskResult.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Write task results to a path.</p> Source code in <code>src/seb/result_dataclasses.py</code> <pre><code>def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n    Write task results to a path.\n    \"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n    json_str: str = self.model_dump_json()\n\n    with path.open(\"w\") as f:\n        f.write(json_str)\n</code></pre>"},{"location":"api/#seb.TaskError","title":"<code>seb.TaskError</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>src/seb/result_dataclasses.py</code> <pre><code>class TaskError(BaseModel):\n    task_name: str\n    error: str\n    time_of_run: datetime\n    languages: list[str] = []\n\n    def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n        Write task results to a path.\n        \"\"\"\n        path.parent.mkdir(parents=True, exist_ok=True)\n        json_str: str = self.model_dump_json()\n\n        with path.open(\"w\") as f:\n            f.write(json_str)\n\n    @classmethod\n    def from_disk(cls, path: Path) -&gt; \"TaskError\":  # noqa: ANN102\n\"\"\"\n        Load task results from a path.\n        \"\"\"\n        with path.open() as f:\n            task_results = json.load(f)\n        return cls(**task_results)\n\n    @staticmethod\n    def get_main_score(lang: Optional[Iterable[str]] = None) -&gt; float:  # noqa: ARG004\n        return np.nan\n</code></pre>"},{"location":"api/#seb.TaskError.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Load task results from a path.</p> Source code in <code>src/seb/result_dataclasses.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path) -&gt; \"TaskError\":  # noqa: ANN102\n\"\"\"\n    Load task results from a path.\n    \"\"\"\n    with path.open() as f:\n        task_results = json.load(f)\n    return cls(**task_results)\n</code></pre>"},{"location":"api/#seb.TaskError.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Write task results to a path.</p> Source code in <code>src/seb/result_dataclasses.py</code> <pre><code>def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n    Write task results to a path.\n    \"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n    json_str: str = self.model_dump_json()\n\n    with path.open(\"w\") as f:\n        f.write(json_str)\n</code></pre>"},{"location":"getting_started/","title":"Getting started","text":"In\u00a0[6]: Copied! <pre>import seb\n\ntasks = [\"DKHate\"]\nmodel = seb.get_model(\"jonfd/electra-small-nordic\")\n\n# initialize benchmark with tasks\nbenchmark = seb.Benchmark(tasks=tasks)\n\n# benchmark the model\nbenchmark_result = benchmark.evaluate_model(model)\n</pre> import seb  tasks = [\"DKHate\"] model = seb.get_model(\"jonfd/electra-small-nordic\")  # initialize benchmark with tasks benchmark = seb.Benchmark(tasks=tasks)  # benchmark the model benchmark_result = benchmark.evaluate_model(model) In\u00a0[7]: Copied! <pre>benchmark_result  # examine output\n</pre> benchmark_result  # examine output Out[7]: <pre>BenchmarkResults(meta=ModelMeta(name='electra-small-nordic', description=None, huggingface_name='jonfd/electra-small-nordic', reference='https://huggingface.co/{hf_name}', languages=['da', 'no', 'sv']), task_results=[TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='59d12749a3c91a186063c7d729ec392fda94681c_1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 27, 13, 21, 43, 861342), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.15442320525050762, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.019081572459727296, 'main_score': 0.5945288753799393}}, main_score='accuracy')])</pre> In\u00a0[8]: Copied! <pre>benchmark_result[0] # examine the results for the first task\n</pre> benchmark_result[0] # examine the results for the first task Out[8]: <pre>TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='59d12749a3c91a186063c7d729ec392fda94681c_1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 27, 13, 21, 43, 861342), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.15442320525050762, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.019081572459727296, 'main_score': 0.5945288753799393}}, main_score='accuracy')</pre> In\u00a0[1]: Copied! <pre>import seb\n\nmodel_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n\ndef get_my_model():\n    from sentence_transformers import SentenceTransformer\n\n    return SentenceTransformer(model_name)\n\n\n@seb.models.register(model_name)\ndef create_all_mini_lm_l6_v2() -&gt; seb.SebModel:\n    hf_name = model_name\n\n    # create meta data\n    meta = seb.ModelMeta(\n        name=hf_name.split(\"/\")[-1],\n        huggingface_name=hf_name,\n        reference=\"https://huggingface.co/{hf_name}\",\n        languages=[],\n    )\n    return seb.SebModel(\n        loader=get_my_model,\n        meta=meta,\n    )\n</pre> import seb  model_name = \"sentence-transformers/all-MiniLM-L12-v2\"  def get_my_model():     from sentence_transformers import SentenceTransformer      return SentenceTransformer(model_name)   @seb.models.register(model_name) def create_all_mini_lm_l6_v2() -&gt; seb.SebModel:     hf_name = model_name      # create meta data     meta = seb.ModelMeta(         name=hf_name.split(\"/\")[-1],         huggingface_name=hf_name,         reference=\"https://huggingface.co/{hf_name}\",         languages=[],     )     return seb.SebModel(         loader=get_my_model,         meta=meta,     ) In\u00a0[\u00a0]: Copied! <pre># deliberately not running this test as it takes a while\nfrom seb import run_benchmark\n\nresults = run_benchmark()\n</pre> # deliberately not running this test as it takes a while from seb import run_benchmark  results = run_benchmark() <p>This runs the full benchmark on all the registrered models as well as all the registrered datasets. The results are returned as a dictionary of where the keys represent the benchmark and values are a list of benchmark results.</p>"},{"location":"getting_started/#getting-started","title":"Getting started\u00b6","text":"<p>This is a minimal documentation at the moment at I am unsure how many will use the package. If you do want to use the package, but feel like the documentation is lacking feel free to open an issue on GitHub.</p>"},{"location":"getting_started/#running-a-task","title":"Running a task\u00b6","text":"<p>To run a task you will need to fetch the task, a model run it.</p>"},{"location":"getting_started/#adding-a-model","title":"Adding a model\u00b6","text":"<p>The benchmark uses a registry to add models. A model in <code>seb</code> includes two thing. 1) a metadata object (<code>seb.ModelMeta</code>) describing the metadata of the model and 2) a loader for the model itself, which is an object that needs an encode methods as described by the <code>seb.ModelInterface</code>. Here is an example of how to add a model:</p>"},{"location":"getting_started/#reproducing-the-benchmark","title":"Reproducing the Benchmark\u00b6","text":"<p>Reproducing the benchmark is easy and is doable simply using the following command:</p>"},{"location":"installation/","title":"Installation","text":"<p>You can install the <code>seb</code> via pip from PyPI:</p> <pre><code>pip install seb\n</code></pre> <p>or from GitHub using:</p> <pre><code>pip install git+https://github.com/KennethEnevoldsen/scandinavian-embedding-benchmark\n</code></pre>"},{"location":"run_benchmark/","title":"Run benchmark","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript for running the benchmark and pushing the results to Datawrapper.\n\nExample:\n\n    python run_benchmark.py --data-wrapper-api-token &lt;token&gt;\n\"\"\"\nimport argparse\n</pre> \"\"\" Script for running the benchmark and pushing the results to Datawrapper.  Example:      python run_benchmark.py --data-wrapper-api-token  \"\"\" import argparse In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport seb\nfrom datawrapper import Datawrapper\nfrom seb.full_benchmark import BENCHMARKS\n</pre> import numpy as np import pandas as pd import seb from datawrapper import Datawrapper from seb.full_benchmark import BENCHMARKS In\u00a0[\u00a0]: Copied! <pre>subset_to_chart_id = {\n    \"Mainland Scandinavian\": \"7Nwjx\",\n    \"Danish\": \"us1YK\",\n    \"Norwegian\": \"pV87q\",\n    \"Swedish\": \"aL23t\",\n}\n</pre> subset_to_chart_id = {     \"Mainland Scandinavian\": \"7Nwjx\",     \"Danish\": \"us1YK\",     \"Norwegian\": \"pV87q\",     \"Swedish\": \"aL23t\", } In\u00a0[\u00a0]: Copied! <pre>datawrapper_lang_codes = {\n    \"da\": \"dk\",\n    \"nb\": \"no\",\n    \"sv\": \"se\",\n    \"en\": \"us\",\n}\n</pre> datawrapper_lang_codes = {     \"da\": \"dk\",     \"nb\": \"no\",     \"sv\": \"se\",     \"en\": \"us\", } In\u00a0[\u00a0]: Copied! <pre>def get_main_score(task: seb.TaskResult, langs: list[str]) -&gt; float:\n    _langs = set(langs) &amp; set(task.languages)\n    return task.get_main_score(_langs) * 100\n</pre> def get_main_score(task: seb.TaskResult, langs: list[str]) -&gt; float:     _langs = set(langs) &amp; set(task.languages)     return task.get_main_score(_langs) * 100 In\u00a0[\u00a0]: Copied! <pre>def create_mdl_name(mdl: seb.ModelMeta) -&gt; str:\n    reference = mdl.reference\n    name = mdl.name\n\n    mdl_name = f\"[{name}]({reference})\" if reference else name\n\n    if mdl.languages:\n        lang_code = \" \".join(\n            [\n                f\":{datawrapper_lang_codes[l]}:\"\n                for l in mdl.languages\n                if l in datawrapper_lang_codes\n            ],\n        )\n        mdl_name = f\"{mdl_name} {lang_code}\"\n\n    return mdl_name\n</pre> def create_mdl_name(mdl: seb.ModelMeta) -&gt; str:     reference = mdl.reference     name = mdl.name      mdl_name = f\"[{name}]({reference})\" if reference else name      if mdl.languages:         lang_code = \" \".join(             [                 f\":{datawrapper_lang_codes[l]}:\"                 for l in mdl.languages                 if l in datawrapper_lang_codes             ],         )         mdl_name = f\"{mdl_name} {lang_code}\"      return mdl_name In\u00a0[\u00a0]: Copied! <pre>def benchmark_result_to_row(\n    result: seb.BenchmarkResults,\n    langs: list[str],\n) -&gt; pd.DataFrame:\n    mdl_name = create_mdl_name(result.meta)\n    # sort by task name\n    task_results = result.task_results\n    sorted_tasks = sorted(task_results, key=lambda t: t.task_name)\n    task_names = [t.task_name for t in sorted_tasks]\n    scores = [get_main_score(t, langs) for t in sorted_tasks]  # type: ignore\n\n    df = pd.DataFrame([scores], columns=task_names, index=[mdl_name])\n    df[\"Average\"] = np.mean(scores)  # type: ignore\n    return df\n</pre> def benchmark_result_to_row(     result: seb.BenchmarkResults,     langs: list[str], ) -&gt; pd.DataFrame:     mdl_name = create_mdl_name(result.meta)     # sort by task name     task_results = result.task_results     sorted_tasks = sorted(task_results, key=lambda t: t.task_name)     task_names = [t.task_name for t in sorted_tasks]     scores = [get_main_score(t, langs) for t in sorted_tasks]  # type: ignore      df = pd.DataFrame([scores], columns=task_names, index=[mdl_name])     df[\"Average\"] = np.mean(scores)  # type: ignore     return df In\u00a0[\u00a0]: Copied! <pre>def convert_to_table(\n    results: list[seb.BenchmarkResults],\n    langs: list[str],\n) -&gt; pd.DataFrame:\n    rows = [benchmark_result_to_row(result, langs) for result in results]\n    df = pd.concat(rows)\n    df = df.sort_values(by=\"Average\", ascending=False)\n\n    # ensure that the average first column\n    cols = df.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    df = df[cols]\n\n    # convert name to column\n    df = df.reset_index()\n    df = df.rename(columns={\"index\": \"Model\"})\n\n    return df\n</pre> def convert_to_table(     results: list[seb.BenchmarkResults],     langs: list[str], ) -&gt; pd.DataFrame:     rows = [benchmark_result_to_row(result, langs) for result in results]     df = pd.concat(rows)     df = df.sort_values(by=\"Average\", ascending=False)      # ensure that the average first column     cols = df.columns.tolist()     cols = cols[-1:] + cols[:-1]     df = df[cols]      # convert name to column     df = df.reset_index()     df = df.rename(columns={\"index\": \"Model\"})      return df In\u00a0[\u00a0]: Copied! <pre>def push_to_datawrapper(df: pd.DataFrame, chart_id: str, token: str):\n    dw = Datawrapper(access_token=token)\n    assert dw.account_info(), \"Could not connect to Datawrapper\"\n    resp = dw.add_data(chart_id, data=df)\n    assert 200 &lt;= resp.status_code &lt; 300, \"Could not add data to Datawrapper\"\n    iframe_html = dw.publish_chart(chart_id)\n    assert iframe_html, \"Could not publish chart\"\n</pre> def push_to_datawrapper(df: pd.DataFrame, chart_id: str, token: str):     dw = Datawrapper(access_token=token)     assert dw.account_info(), \"Could not connect to Datawrapper\"     resp = dw.add_data(chart_id, data=df)     assert 200 &lt;= resp.status_code &lt; 300, \"Could not add data to Datawrapper\"     iframe_html = dw.publish_chart(chart_id)     assert iframe_html, \"Could not publish chart\" In\u00a0[\u00a0]: Copied! <pre>def main(data_wrapper_api_token: str):\n    results = seb.run_benchmark(use_cache=True)\n\n    for subset, result in results.items():\n        langs = BENCHMARKS[subset]\n\n        table = convert_to_table(result, langs)\n        chart_id = subset_to_chart_id[subset]\n        push_to_datawrapper(table, chart_id, data_wrapper_api_token)\n</pre> def main(data_wrapper_api_token: str):     results = seb.run_benchmark(use_cache=True)      for subset, result in results.items():         langs = BENCHMARKS[subset]          table = convert_to_table(result, langs)         chart_id = subset_to_chart_id[subset]         push_to_datawrapper(table, chart_id, data_wrapper_api_token) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--data-wrapper-api-token\",\n        type=str,\n        required=True,\n        help=\"Datawrapper API token\",\n    )\n\n    args = parser.parse_args()\n    main(args.data_wrapper_api_token)\n</pre> if __name__ == \"__main__\":     parser = argparse.ArgumentParser()     parser.add_argument(         \"--data-wrapper-api-token\",         type=str,         required=True,         help=\"Datawrapper API token\",     )      args = parser.parse_args()     main(args.data_wrapper_api_token)"}]}